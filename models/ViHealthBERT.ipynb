{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets seqeval accelerate torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Model configuration\n",
    "    MODEL_NAME = \"demdecuong/vihealthbert-base-word\"  # VihealthBERT model\n",
    "\n",
    "    # Data paths\n",
    "    TRAIN_FILE = \"/content/drive/MyDrive/data_ner/train.txt\"\n",
    "    DEV_FILE = \"/content/drive/MyDrive/data_ner/dev.txt\"\n",
    "    TEST_FILE = \"/content/drive/MyDrive/data_ner/test.txt\"\n",
    "\n",
    "    # Training parameters\n",
    "    OUTPUT_DIR = \"./vihealth_ner_model\"\n",
    "    LEARNING_RATE = 2e-5\n",
    "    BATCH_SIZE = 12\n",
    "    NUM_EPOCHS = 5\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    WARMUP_RATIO = 0.1\n",
    "    MAX_LENGTH = 256\n",
    "\n",
    "    # Other settings\n",
    "    SEED = 42\n",
    "    SAVE_STEPS = 500\n",
    "    EVAL_STEPS = 500\n",
    "    LOGGING_STEPS = 100\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(config.SEED)\n",
    "np.random.seed(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CoNLL datasets...\n",
      "Train samples: 6199\n",
      "Dev samples: 774\n",
      "Test samples: 776\n"
     ]
    }
   ],
   "source": [
    "def read_conll_file(file_path):\n",
    "    \"\"\"\n",
    "    Read CoNLL format file and return sentences and labels\n",
    "    Expected format:\n",
    "    Token Label\n",
    "    Token Label\n",
    "    (blank line separates sentences)\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line == \"\" or line.startswith(\"-DOCSTART-\"):\n",
    "                if current_tokens:\n",
    "                    sentences.append(current_tokens)\n",
    "                    labels.append(current_labels)\n",
    "                    current_tokens = []\n",
    "                    current_labels = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    token = parts[0]\n",
    "                    label = parts[-1]  # Last column is label\n",
    "                    current_tokens.append(token)\n",
    "                    current_labels.append(label)\n",
    "\n",
    "        # Add last sentence if exists\n",
    "        if current_tokens:\n",
    "            sentences.append(current_tokens)\n",
    "            labels.append(current_labels)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "print(\"Loading CoNLL datasets...\")\n",
    "train_tokens, train_labels = read_conll_file(config.TRAIN_FILE)\n",
    "dev_tokens, dev_labels = read_conll_file(config.DEV_FILE)\n",
    "test_tokens, test_labels = read_conll_file(config.TEST_FILE)\n",
    "\n",
    "print(f\"Train samples: {len(train_tokens)}\")\n",
    "print(f\"Dev samples: {len(dev_tokens)}\")\n",
    "print(f\"Test samples: {len(test_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of labels: 11\n",
      "Labels: ['B-bien_phap_chan_doan', 'B-bien_phap_dieu_tri', 'B-nguyen_nhan_benh', 'B-ten_benh', 'B-trieu_chung_benh', 'I-bien_phap_chan_doan', 'I-bien_phap_dieu_tri', 'I-nguyen_nhan_benh', 'I-ten_benh', 'I-trieu_chung_benh', 'O']\n"
     ]
    }
   ],
   "source": [
    "all_labels = set()\n",
    "for labels in train_labels + dev_labels + test_labels:\n",
    "    all_labels.update(labels)\n",
    "\n",
    "label_list = sorted(list(all_labels))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "num_labels = len(label_list)\n",
    "print(f\"\\nNumber of labels: {num_labels}\")\n",
    "print(f\"Labels: {label_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 6199\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 774\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 776\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample from training set:\n",
      "{'tokens': ['các', 'biện', 'pháp', 'điều', 'trị', 'bệnh', 'ung', 'thư', 'đại', 'tràng', 'phương', 'pháp', 'điều', 'trị', 'được', 'quyết', 'định', 'dựa', 'trên', 'giai', 'đoạn', 'của', 'ung', 'thư', 'đại', 'tràng', 'giai', 'đoạn', 'i', 'đến', 'iiia', ':', 'thông', 'thường', 'có', 'thể', 'được', 'điều', 'trị', 'bằng', 'phẫu', 'thuật', 'cắt', 'bỏ', 'khối', 'u', 'giai', 'đoạn', 'iiib', 'hoặc', 'iiic', ':', 'hóa', 'trị', 'kèm', 'theo', 'phẫu', 'thuật', 'để', 'ngăn', 'ngừa', 'các', 'tế', 'bào', 'ung', 'thư', 'tấn', 'công', 'các', 'cơ', 'quan', 'khác', 'của', 'cơ', 'thể', 'giai', 'đoạn', 'iv', ':', 'hóa', 'trị', 'là', 'phương', 'pháp', 'hiệu', 'quả', 'để', 'điều', 'trị', 'ung', 'thư', 'phẫu', 'thuật', ':', 'phẫu', 'thuật', 'dự', 'phòng', 'bệnh', ':', 'phẫu', 'thuật', 'cắt', 'bỏ', 'những', 'thương', 'tổn', 'tiền', 'ung', 'thư', 'góp', 'phần', 'tích', 'cực', 'làm', 'hạ', 'thấp', 'tỷ', 'lệ', 'mắc', 'bệnh', 'phẫu', 'thuật', 'điều', 'trị', 'ung', 'thư', ':', 'có', 'hai', 'loại', 'chỉ', 'định', 'chính', 'là', 'phẫu', 'thuật', 'triệt', 'để', 'và', 'tạm', 'thời', '.'], 'ner_tags': [10, 10, 10, 10, 10, 3, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 3, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1, 6, 6, 6, 6, 6, 10, 10, 10, 10, 10, 10, 1, 6, 10, 10, 1, 6, 10, 10, 10, 10, 10, 10, 3, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1, 6, 10, 10, 10, 10, 10, 10, 10, 10, 3, 8, 6, 6, 10, 1, 6, 10, 10, 10, 10, 1, 6, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1, 6, 10, 10, 3, 8, 10, 10, 10, 10, 10, 10, 10, 10, 1, 6, 10, 10, 10, 10, 10, 10]}\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(tokens, labels):\n",
    "    \"\"\"Convert tokens and labels to dataset format\"\"\"\n",
    "    return Dataset.from_dict({\n",
    "        'tokens': tokens,\n",
    "        'ner_tags': [[label2id[label] for label in sent_labels] for sent_labels in labels]\n",
    "    })\n",
    "\n",
    "train_dataset = create_dataset(train_tokens, train_labels)\n",
    "dev_dataset = create_dataset(dev_tokens, dev_labels)\n",
    "test_dataset = create_dataset(test_tokens, test_labels)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': dev_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "print(\"\\nSample from training set:\")\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31fde5c3d75c44259a99f74b6947c25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/817 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c4dcc11e0d4f099005eb5c2fcdd1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf01656895394e879b1aad4887a35f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4881b4caa5da4680af99950117a3b849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at demdecuong/vihealthbert-base-word and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    config.MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bfdc0767a9640c487985950e85f944b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b14ba1098494cfd9cfc26737d0e25b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2de4aa68ff846189813109172f6d03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1b5dc23d954551bbc56b94d56ee567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/776 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenize inputs and align labels with tokens\n",
    "    Compatible with non-fast tokenizers (VihealthBERT)\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "\n",
    "    for tokens, ner_tags in zip(examples['tokens'], examples['ner_tags']):\n",
    "        # Tokenize each word individually\n",
    "        tokenized_words = []\n",
    "        label_ids = []\n",
    "\n",
    "        # Add CLS token\n",
    "        tokenized_words.append(tokenizer.cls_token_id)\n",
    "        label_ids.append(-100)\n",
    "\n",
    "        # Process each word\n",
    "        for word, label in zip(tokens, ner_tags):\n",
    "            # Tokenize the word\n",
    "            word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "\n",
    "            # Truncate if needed\n",
    "            if len(tokenized_words) + len(word_tokens) + 1 > config.MAX_LENGTH:\n",
    "                break\n",
    "\n",
    "            # Add tokens\n",
    "            tokenized_words.extend(word_tokens)\n",
    "\n",
    "            # Assign label to first subword, -100 to others\n",
    "            label_ids.append(label)\n",
    "            label_ids.extend([-100] * (len(word_tokens) - 1))\n",
    "\n",
    "        # Add SEP token\n",
    "        tokenized_words.append(tokenizer.sep_token_id)\n",
    "        label_ids.append(-100)\n",
    "\n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(tokenized_words)\n",
    "\n",
    "        input_ids_list.append(tokenized_words)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_list,\n",
    "        'attention_mask': attention_mask_list,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "print(\"\\nTokenizing datasets...\")\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute precision, recall, and F1 score using seqeval\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        true_label = []\n",
    "        true_prediction = []\n",
    "\n",
    "        for pred_id, label_id in zip(prediction, label):\n",
    "            if label_id != -100:\n",
    "                true_label.append(id2label[label_id])\n",
    "                true_prediction.append(id2label[pred_id])\n",
    "\n",
    "        true_labels.append(true_label)\n",
    "        true_predictions.append(true_prediction)\n",
    "\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=config.OUTPUT_DIR,\n",
    "    eval_strategy=\"steps\",  # Changed from evaluation_strategy\n",
    "    eval_steps=config.EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=config.SAVE_STEPS,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "    num_train_epochs=config.NUM_EPOCHS,\n",
    "    weight_decay=config.WEIGHT_DECAY,\n",
    "    warmup_ratio=config.WARMUP_RATIO,\n",
    "    logging_steps=config.LOGGING_STEPS,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    push_to_hub=False,\n",
    "    seed=config.SEED,\n",
    "    save_total_limit=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3348431065.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STARTING TRAINING\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2585' max='2585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2585/2585 12:23, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.239800</td>\n",
       "      <td>0.369955</td>\n",
       "      <td>0.551813</td>\n",
       "      <td>0.726608</td>\n",
       "      <td>0.627261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>0.352535</td>\n",
       "      <td>0.604368</td>\n",
       "      <td>0.701267</td>\n",
       "      <td>0.649222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.177900</td>\n",
       "      <td>0.355268</td>\n",
       "      <td>0.610220</td>\n",
       "      <td>0.704191</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.150800</td>\n",
       "      <td>0.368844</td>\n",
       "      <td>0.590381</td>\n",
       "      <td>0.717836</td>\n",
       "      <td>0.647900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>0.381566</td>\n",
       "      <td>0.620704</td>\n",
       "      <td>0.712963</td>\n",
       "      <td>0.663643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TRAINING COMPLETED\n",
      "==================================================\n",
      "\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  total_flos               =  1433136GF\n",
      "  train_loss               =     0.1843\n",
      "  train_runtime            = 0:12:23.92\n",
      "  train_samples_per_second =     41.664\n",
      "  train_steps_per_second   =      3.475\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Save training metrics\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATING ON VALIDATION SET\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "eval_loss: 0.3816\n",
      "eval_precision: 0.6207\n",
      "eval_recall: 0.7130\n",
      "eval_f1: 0.6636\n",
      "eval_runtime: 4.7639\n",
      "eval_samples_per_second: 162.4710\n",
      "eval_steps_per_second: 13.6440\n",
      "epoch: 5.0000\n",
      "***** eval metrics *****\n",
      "  epoch                   =        5.0\n",
      "  eval_f1                 =     0.6636\n",
      "  eval_loss               =     0.3816\n",
      "  eval_precision          =     0.6207\n",
      "  eval_recall             =      0.713\n",
      "  eval_runtime            = 0:00:04.76\n",
      "  eval_samples_per_second =    162.471\n",
      "  eval_steps_per_second   =     13.644\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATING ON VALIDATION SET\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "val_metrics = trainer.evaluate(eval_dataset=tokenized_datasets['validation'])\n",
    "print(\"Validation Metrics:\")\n",
    "for key, value in val_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "trainer.log_metrics(\"eval\", val_metrics)\n",
    "trainer.save_metrics(\"eval\", val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATING ON TEST SET\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:\n",
      "eval_loss: 0.4361\n",
      "eval_precision: 0.6333\n",
      "eval_recall: 0.6880\n",
      "eval_f1: 0.6595\n",
      "eval_runtime: 4.3210\n",
      "eval_samples_per_second: 179.5890\n",
      "eval_steps_per_second: 15.0430\n",
      "epoch: 5.0000\n",
      "***** test metrics *****\n",
      "  epoch                   =        5.0\n",
      "  eval_f1                 =     0.6595\n",
      "  eval_loss               =     0.4361\n",
      "  eval_precision          =     0.6333\n",
      "  eval_recall             =      0.688\n",
      "  eval_runtime            = 0:00:04.32\n",
      "  eval_samples_per_second =    179.589\n",
      "  eval_steps_per_second   =     15.043\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "test_metrics = trainer.evaluate(eval_dataset=tokenized_datasets['test'])\n",
    "print(\"Test Metrics:\")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "trainer.log_metrics(\"test\", test_metrics)\n",
    "trainer.save_metrics(\"test\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DETAILED CLASSIFICATION REPORT (TEST SET)\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "bien_phap_chan_doan       0.54      0.64      0.59       130\n",
      " bien_phap_dieu_tri       0.50      0.54      0.52       239\n",
      "   nguyen_nhan_benh       0.27      0.31      0.29       157\n",
      "           ten_benh       0.78      0.83      0.80       922\n",
      "   trieu_chung_benh       0.56      0.62      0.59       382\n",
      "\n",
      "          micro avg       0.63      0.69      0.66      1830\n",
      "          macro avg       0.53      0.59      0.56      1830\n",
      "       weighted avg       0.64      0.69      0.66      1830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DETAILED CLASSIFICATION REPORT (TEST SET)\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "predictions = trainer.predict(tokenized_datasets['test'])\n",
    "preds = np.argmax(predictions.predictions, axis=2)\n",
    "\n",
    "true_labels = []\n",
    "true_predictions = []\n",
    "\n",
    "for prediction, label in zip(preds, predictions.label_ids):\n",
    "    true_label = []\n",
    "    true_prediction = []\n",
    "\n",
    "    for pred_id, label_id in zip(prediction, label):\n",
    "        if label_id != -100:\n",
    "            true_label.append(id2label[label_id])\n",
    "            true_prediction.append(id2label[pred_id])\n",
    "\n",
    "    true_labels.append(true_label)\n",
    "    true_predictions.append(true_prediction)\n",
    "\n",
    "print(classification_report(true_labels, true_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SAVING MODEL\n",
      "==================================================\n",
      "\n",
      "Model saved to: ./vihealth_ner_model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "trainer.save_model(config.OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(config.OUTPUT_DIR)\n",
    "\n",
    "# Save label mapping\n",
    "with open(os.path.join(config.OUTPUT_DIR, 'label_mapping.json'), 'w') as f:\n",
    "    json.dump({'label2id': label2id, 'id2label': id2label}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Model saved to: {config.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "INFERENCE EXAMPLE\n",
      "==================================================\n",
      "\n",
      "Input: Bệnh nhân bị đau đầu và sốt cao\n",
      "\n",
      "Predictions:\n",
      "<s>                  -> O\n",
      "Bệnh                 -> O\n",
      "nhân                 -> O\n",
      "bị                   -> O\n",
      "đau                  -> B-trieu_chung_benh\n",
      "đầu                  -> I-trieu_chung_benh\n",
      "và                   -> O\n",
      "sốt                  -> B-trieu_chung_benh\n",
      "cao                  -> I-trieu_chung_benh\n",
      "</s>                 -> O\n",
      "\n",
      "==================================================\n",
      "TRAINING PIPELINE COMPLETED!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INFERENCE EXAMPLE\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def predict_ner(text):\n",
    "    \"\"\"Predict NER tags for input text\"\"\"\n",
    "    # Tokenize và đưa input lên cùng device\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=config.MAX_LENGTH,\n",
    "        is_split_into_words=False\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # 🔥 dòng quan trọng\n",
    "\n",
    "    # Dự đoán\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "    results = []\n",
    "    for token, pred in zip(tokens, predictions[0]):\n",
    "        if token not in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            results.append((token, id2label[pred.item()]))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test\n",
    "test_text = \"Bệnh nhân bị đau đầu và sốt cao\"\n",
    "predictions = predict_ner(test_text)\n",
    "\n",
    "print(f\"Input: {test_text}\")\n",
    "print(\"\\nPredictions:\")\n",
    "for token, label in predictions:\n",
    "    print(f\"{token:20} -> {label}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING PIPELINE COMPLETED!\")\n",
    "print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
